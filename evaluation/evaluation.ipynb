{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-fold cross validation\n",
    "\n",
    "Evaluate a model trained with k-fold cross validation where each site is treated as a separate fold. Predictions from all test folds are combined and the performance is evaluated against the entire ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "from tide.tidecv import TIDE, datasets\n",
    "\n",
    "SPLIT = 'test'\n",
    "PREDICTIONS_PATH = '/path/to/kfold_results/RUN_ID'\n",
    "GT_PATH = '/path/to/dronewaste/dronewaste_v1.0.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare ground truth and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load global gt\n",
    "with open(GT_PATH, 'r') as f:\n",
    "    global_gt = json.load(f)\n",
    "\n",
    "# map image filename to image id\n",
    "mapping = {\n",
    "    img['file_name'].split('.')[0]: img['id'] for img in global_gt['images']\n",
    "}\n",
    "\n",
    "# enumerate sites in the global gt\n",
    "sites_ggt = {img['site'] for img in global_gt['images']}\n",
    "\n",
    "# create folder for temporary site gts\n",
    "tmp_path = os.path.join(PREDICTIONS_PATH, 'tmp_eval')\n",
    "os.makedirs(tmp_path, exist_ok=True)\n",
    "\n",
    "\n",
    "# validate prediction files and merge all predictions into one file\n",
    "all_predictions = []\n",
    "sites_prds = set()\n",
    "\n",
    "# enumerate all folders (one for each fold)\n",
    "folders = [\n",
    "    f for f in os.listdir(PREDICTIONS_PATH)\n",
    "    if os.path.isdir(os.path.join(PREDICTIONS_PATH, f))\n",
    "    and f != 'tmp_eval'\n",
    "]\n",
    "\n",
    "for folder in folders:\n",
    "    pred_file = os.path.join(PREDICTIONS_PATH, folder, SPLIT, 'predictions.json')\n",
    "    with open(pred_file, 'r') as f:\n",
    "        site_predictions = json.load(f)\n",
    "\n",
    "    # extract site name ->Â e.g. \"site1_100\"\n",
    "    curr_site = site_predictions[0]['image_id'].split('_')[0]\n",
    "\n",
    "    # check that all predictions are from the same site\n",
    "    for pred in site_predictions:\n",
    "        assert pred['image_id'].split('_')[0] == curr_site, f'{pred[\"image_id\"]} appears in predictions for site: {curr_site}'\n",
    "\n",
    "        # convert image filename to image id\n",
    "        pred['image_id'] = mapping[pred['image_id']]\n",
    "        # for yolo: category id must be shifted by 1\n",
    "        if 'yolo' in PREDICTIONS_PATH:\n",
    "            pred['category_id'] = pred['category_id'] - 1\n",
    "\n",
    "    sites_prds.add(curr_site)\n",
    "    all_predictions.extend(site_predictions)\n",
    "\n",
    "assert sites_prds == sites_ggt, 'Sites in predictions and ground truth are different'\n",
    "\n",
    "print('validated predictions files')\n",
    "print(f'found {len(sites_prds)} sites')\n",
    "\n",
    "# save merged predictions\n",
    "with open(os.path.join(tmp_path, 'all_predictions.json'), 'w') as f:\n",
    "    json.dump(all_predictions, f)\n",
    "\n",
    "print('merged all predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate global results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load global dataset\n",
    "ds = datasets.COCO(GT_PATH)\n",
    "# load all predictions\n",
    "preds = datasets.COCOResult(\n",
    "    path=os.path.join(tmp_path, 'all_predictions.json'),\n",
    "    name='preds',\n",
    ")\n",
    "\n",
    "tide = TIDE()\n",
    "results = tide.evaluate_range(\n",
    "    gt=ds,\n",
    "    preds=preds,\n",
    "    thresholds=tide.COCO_THRESHOLDS,\n",
    "    mode=TIDE.BOX,\n",
    ")\n",
    "\n",
    "# extract mAP all thresholds\n",
    "runs = tide.run_thresholds['preds']\n",
    "\n",
    "metrics = {f'mAP@{int(trun.pos_thresh*100)}': trun.ap for trun in runs}\n",
    "metrics['mAP@50-95'] = np.mean(list(metrics.values()))\n",
    "\n",
    "# extract mAP per class\n",
    "tide_run = tide.runs['preds']\n",
    "class_aps = [0.0] * len(ds.classes)\n",
    "for cl, ap in tide_run.per_classes_ap.items():\n",
    "    class_aps[cl] = ap\n",
    "\n",
    "header = ['mAP@50-95', 'mAP@50'] + [f\"AP_{cls['name']}\" for cls in global_gt['categories']]\n",
    "values = [metrics['mAP@50-95'], metrics['mAP@50']] + class_aps\n",
    "\n",
    "df = pd.DataFrame([values], columns=header)\n",
    "\n",
    "csv_path = os.path.join(PREDICTIONS_PATH, f'{os.path.basename(PREDICTIONS_PATH)}_metrics.csv')\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print('global metrics saved to:', csv_path)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove tmp directory\n",
    "shutil.rmtree(tmp_path)\n",
    "\n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
